\chapter{Introduction}
%Relational databases have been the standard for data storage in a wide range of domains and applications since the 1970s \cite{Codd}. Many attempts have been made to replace them but none have yet to succeeded in a meaningful way \cite{NoSQLDistilled}.
Relational databases have been the standard for data storage in a wide range of domains and applications since the 1970s \cite{Codd}. Many attempts have been made to replace them but none have yet to succeeded in a meaningful way \cite{NoSQLDistilled}. Instead, a contemporary viewpoint is that no data model can and should have the purpose of describing all kinds of domains \cite{NoSQLDistilled, NoSQLSurvey}. In practise this means that trying to model all data to fit into one kind of database, relational or otherwise, is not a prudent task. This problem is known in literature as impedance mismatch \cite{ORM}. Many strategies \cite{ORM} exist for overcoming it where a more recent one is the concept of polyglot persistence \cite{NoSQLDistilled}. The argument for polyglot persistence is simple; Use the right tool for the right job. This means storing data in the form it naturally exists in its domain. One implication is that a system should potentially be made up of multiple databases, where each database is designed for specific kinds of data and to solve specific problems.

Polyglot persistence is closely related to the rise in popularity of alternatives to relational databases, typically classified as NoSQL databases \cite{Catell}. NoSQL simply means "Not Relational". These databases should not be considered general purpose \cite{Catell, polyglotms} but should instead be used for solving specific problems involving specific kinds of data. A typical use case for NoSQL is storing massive scale and potentially unstructured data. This is an area where relational databases historically have a problem \cite{Catell}.

\section{Problem identification}

Organizations often find themselves in a situation where the requirements of a software system have emergent properties that were not planned for. In this thesis, the emergent problem that has been identified is that of unplanned and large data growth. To manage this problem, one of the first responses is often to physically separate stale data from live data \cite{Mullins}. This in turn introduces complexity related to how databases handle historical data when the structure of the stored data is evolving. 

In addition to increased complexity, Curino et al.\ \cite{Moon09} state that the relational model alone is too restrictive for storing and querying evolving historical data. However, polyglot persistence has shown promise in this area \cite{Moon05, Moon09, MoonPHD}. In \cite{Moon09}, a solution is described where XML is used to store historical data along with mappings between schema versions. The solution is reported to provide a unified query interface on all different schema versions of the historical data. However, XML databases are noted to scale poorly \cite{Moon09} and XML as a data format for semi-structured data is considered less than ideal \cite{JSONData}. As a comparison, the JSON format is simpler \cite{JSONData} and more widely adopted by newer NoSQL databases \cite{Catell}.

Given the mentioned issues regarding XML databases, other non relational storage options should be evaluated for storing evolving historical data. The emergence of the NoSQL movement has led to comparative studies \cite{Catell, NoSQLSurvey} which are relevant for this evaluation. This research has later been extended with knowledge on how to use different NoSQL databases in the same system \cite{NoSQLDistilled} and how to use both SQL and NoSQL databases in the same system \cite{MySQLToNoSQL}. However, the research field of databases lack studies investigating the practical suitability of polyglot persistence solutions with evolving historical data.

In this thesis, a solution to archiving of historical data with evolving schemas was developed and evaluated using a design research approach. The practical suitability of a polyglot persistence solution using NoSQL as an alternative was explored in depth. The thesis was conducted at Ericsson as a response to an identified problem with an existing system that aggregates and visualizes data from software build systems and continuous integration workflows.

\section{Business context}
The existing system that proposed solutions from this study will be constructed to extend is called 'CIMS', or continuous integration management system. It was developed in order to support Ericsson in its change towards agile development, including continuous integration workflows. The system has several instances, one for each of the larger products at Ericsson. It is a web application that aggregates and visualizes data related to continuous integration, primarily regarding products, builds, tests and trouble reports.

Recently it has been observed, in part due to the success of CIMS, that the amount of registered builds is increasing steadily. At the same time the amount of test cases for each build is also increasing, making the overall data growth nonlinear. The technology stack of CIMS was originally not made to scale with this kind of growth which in turn has introduced problems that impact performance, maintainability, changeability and other factors.

Many problems with the current implementation stem from the need to evolve the system, specifically its data model and schemas. In the current implementation, all system data is stored in a single relational database. For evolution to take place, all data must be migrated to match new versions of the system. At current rates of stored data, these migrations can take as much as a weeks time to complete. However, a large portion of the data is considered stale as it is not accessed on a daily basis.

A consequence of moving towards continuous integration is certainly that an organization will start to collect much more data about its software development process. The value of this data is defined by how it is used. If it is never used for anything in particular then it should just be purged on a regular basis to keep databases at a manageable size. This is however not likely to be the case. As such, a number of benefits can be observed by solving the data growth problem. Drawbacks also exist.

Benefits: \\
%- Offload live database. By keeping the size of the live database manageable the need for scaling up the hardware it runs on is lessened. It will also be much more performant. The possibility to continuously evolve and deploy CIMS instances is won back. Finally it will be reduced to fulfilling one main task; serving CIMS with data.
- Hardware cost reduction.\\
- Enable continuous integration and deployment of CIMS.\\
- Increased performance of live database.\\
- Make analytics possible.\\
- Full history kept.\\

Drawbacks:\\
- Cost of implementing solution.\\
- Train staff in new technology or hire new staff with the skills needed.\\
- Increased complexity of CIMS and its related systems.\\
- New code to maintain\\

\section{Solution objectives}
Together with the CIMS development team, a set of objectives that define a successful solution has been defined. The main objective is to develop an archive system that physically separates stale data from live data in order to further support the evolution of CIMS.

While stale data is not accessed often, it is still useful for analytics purposes and to respond to trouble reports from customers running older versions of Ericsson products. Therefore a secondary objective is to retain as much functionality as possible in terms of queries on the archived portion of the data. The specific queries that the archive must support has been defined through discussion with the CIMS development team and the metrics team.

Physical separation of live and stale data means that the storage solution for archived data must be flexible enough to handle structure variation on incoming data. If it does not then the problem of long running migrations is not solved but rather moved to the data store of the archive. As such, another objective is to effectively support structural variation of incoming data to the archive.

For the combination of CIMS and the archive system to be future proof it must also be able to scale better with overall data size than CIMS currently does. This ensures that the problem is simply not pushed further into the future but rather solved in its entirety.

In summation, 4 main objectives have been identified. They are as follows;\\
1. Develop an archive solution that physically separates live from stale data.\\
2. Retain a set of important queries on archived data.\\
3. Handle structural variation on incoming data to the archive.\\
4. Introduce long term scalability. \\

\section{Document structure}
This report is structured in the following way; In chapter~\ref{chap:background} the background and related work is described. Chapter~\ref{chap:research} presents the research approach and the chosen research method. Chapter~\ref{chap:domain} describes the domain of CIMS and the business context in more detail. In chapter~\ref{chap:design} an description is given of how the artifact addressing the stated research problem was designed and implemented.
This is followed in chapter~\ref{chap:eval} by a description of how the artifact is evaluated. Results of this evaluation is given in chapter~\ref{chap:results}. The report is then wrapped up in chapters~\ref{chap:discussion} and  ~\ref{chap:conclusion} where results are discussed and conclusions are drawn.

%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.45\linewidth, trim=3cm 11cm 3cm 11cm]{figure/X.pdf}
%\includegraphics[width=0.45\linewidth, trim=3cm 11cm 3cm 11cm]{figure/Y.pdf}
%\caption{Surface and contour plots showing the two dimensional function\\ $z(x,y)=\sin(x+y)\cos(2x), x\in [0,2], y\in [0,1]$.}
%\end{figure}
%
%\section{Equation}
%\begin{equation}
%f(t)=\left\{ \begin{array}{ll}
%1,~~~~ & t< 1 \\
%t^2 & t\geq 1
%\end{array}\right.
%\end{equation}
%
%\section{Table}
%\begin{table}[h!]
%\centering
%\caption{Values of $f(t)$ for $t=0,1,\dots 5$.}
%\begin{tabular}{l|llllll} \hline\hline
%$t$ & 0 & 1 & 2 & 3 & 4 & 5 \\ \hline
%$f(t)$ & 1 & 1 & 4 & 9 & 16 & 25 \\ \hline\hline
%\end{tabular}
%\end{table}
%
%\section{Chemical structure}
%\begin{center}
%\chemfig{X*5(-E-T-A-L-)}
%\end{center}
%
%
%\section{List}
%\begin{enumerate}
%  \item The first item
%  \begin{enumerate}
%    \item Nested item 1
%    \item Nested item 2
%  \end{enumerate}
%  \item The second item
%  \item The third item 
%  \item \dots
%\end{enumerate}
%
%\section{Source code listing}
%%\lstset{language=Matlab}
%\begin{lstlisting}[frame=single]
%% Generate x- and y-nodes
%x=linspace(0,1); y=linspace(0,1);
%
%% Calculate z=f(x,y)
%for i=1:length(x)
% for j=1:length(y)
%  z(i,j)=x(i)+2*y(j);
% end
%end
%\end{lstlisting}

