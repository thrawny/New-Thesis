\chapter{Evaluation}
\label{chap:eval}

In this chapter the different evaluation criteria of the artifact are described and motivated. The artifact is evaluated by metrics that capture the solution objectives and provide relevant data used to address research questions.

%Metrics som rör organisationen skall vi mäta med med gqm \cite{gqm}.   
%Metrics som performance, size on disk, memory consumption skall vi använda oss av ISO standarderna.
 
\section{Data separation}
How well the separation of data works will be evaluated qualitatively by reasoning about the general process of migrating data from the live database in CIMS to the database of the archive. This process is dependent on the effectiveness of the software components that are constructed to support the database of the archive. It is also dependent on how the archive database is configured and how suitable this database is for the task.

The basis for the evaluation is to migrate data sets of different sizes, using data from an existing instance of CIMS. When migration is complete, it is possible to proceed with evaluation points that require data to be present in the archive.  

%This section describes the evaluation of the artifacts' ability to address solution objective 1, namely to develop an archive solution that physically separates live from stale data.

%The measurements are made with data originated from the real world in order to ensure that the artifact is tested in it's real environment.

\section{Query support}
The utility of the archive is dependant on what queries it supports. As a basis for evaluation, the set of important queries defined previously in table~\ref{tab:queries} is used. Evaluation is done by comparing this set to the set of queries that could in fact be implemented in the archive prototypes, using the 'Functional implementation completeness' metric from \cite{isoExternalMetric}.

\section{Schema evolution}
The ability of the archive to handle schema evolution is evaluated by introducing realistic schema variation on incoming data to the archive. These changes are defined in collaboration with the CIMS development team and should capture the planned evolution of the database schema in CIMS.

This evaluation is qualitative and is based on the "Adaptability of data structures" metric defined in \cite{isoInternalMetric}.   

\section{Scalability}
%This evaluation will be done both quantitatively and qualitatively. The quantitative evaluation is done by measuring the size of the data in the archive after a migration. This value will then be compared to how much space this data took in the live database. Qualitatively, each data store's underlying ability to scale will be evaluated in means of which hardware and configuration is needed with a given total data size. Specifically, how much RAM is needed for specific data set sizes.

\subsection{Disk usage}
For the archive to scale, disk usage used must be manageable, even with billions of documents in the archive. The metric used is derived from 'Memory utilization' defined in \cite{isoInternalMetric}, memory is in this case the disk space used by the database of the archive. In order to see the archives ability for long term scalability compared to the live database, measurements is done on both migrated data in the archive and the equivalent data in the live database.              

\subsection{Memory utilization}
The used metric originates from the internal metric defined as 'Maximum memory utilization' in ISO/IEC standard TR 9126-2 \cite{isoExternalMetric}. For the specific case of database applications, this metric can be interpreted as the size of the working set, meaning the amount of data that a database must keep in RAM in order to effectively perform a specific task. The working set is dependant on the kinds of queries that are typically performed as well as the overall disc usage of the database. Comparison is made of the estimated working sets for each of the databases used for implementing the archive against the working set of the relational database used by CIMS.

\subsection{Insertion time}
For the archive to be viable in practice, insertion of new documents into the archive must be efficient. Even with large number of documents already in the archive, the insertions must be made in adequate time. Measurements is be done on insertions of data originated from the live database, the metric will be applied on artifacts containing different amount of data.

\subsection{Horizontal scalability}
The ability of the archive to scale horizontally is evaluated by qualitatively investigate each data stores' capability of scalability. In this case, capability is defined as the data store's characteristics in the following areas: easiness in adding new servers to the cluster, the amount of configuration needed for starting a cluster, and the flexibility in scaling meaning how much information of the data growth that is needed on beforehand.  
