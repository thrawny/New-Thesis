\chapter{Evaluation}
\label{chap:eval}

In this chapter the different evaluation criteria of the artifact are described and motivated. The artifact is evaluated by metrics that capture the solution objectives and provide relevant data used to address research questions.

\section{Data separation}
How well the separation of data works will be evaluated qualitatively by reasoning about the general process of migrating data from the live database in CIMS to the database of the archive. This process is dependent on the effectiveness of the software components that are constructed to support the database of the archive.
A crucial part of this process is the transformation and denormalization of relational data. This transformation is the foundation for how queries can be written against the archive. Depending on how the data is modeled in the archive, operations on that data will have different complexites. The process of separation is also dependent on how the archive database is configured and how suitable it is for the task. 

The basis for the evaluation is to migrate data sets of different sizes, using data from an existing instance of CIMS. This data will also be extended with generated dummy data in order to get a data set that is closer in size to what an archive would be after being put into production given the current data growth trends. When migration is complete, it is possible to proceed with evaluation points that require data to be present in the archive.  

\section{Query support}
The utility of the archive is dependant on what queries it supports and how complex these queries are. As a basis for evaluation, the set of important queries defined previously in section~\ref{sec:usecases} is used. Evaluation is done by comparing this set to the set of queries implemented in the archive prototypes which are previously described in section~\ref{sec:archiveapi}. Comparison is made on the difference in the queries complexity. 

%\section{Schema evolution}
%The ability of the archive to handle schema evolution is evaluated by introducing realistic schema variation on incoming data to the archive. These changes are defined in collaboration with the CIMS development team and should capture the planned evolution of the database schema in CIMS.
%
%This evaluation is qualitative and is based on the "Adaptability of data structures" metric defined in \cite{isoInternalMetric}.

\section{Scalability}
%This evaluation will be done both quantitatively and qualitatively. The quantitative evaluation is done by measuring the size of the data in the archive after a migration. This value will then be compared to how much space this data took in the live database. Qualitatively, each data store's underlying ability to scale will be evaluated in means of which hardware and configuration is needed with a given total data size. Specifically, how much RAM is needed for specific data set sizes.

\subsection{Disk usage}
For the archive to scale, disk usage used must be manageable, even with billions of documents in the archive. The metric used is derived from 'Memory utilization' defined in \cite{isoInternalMetric}, memory is in this case the disk space used by the database of the archive. In order to see the archives ability for long term scalability compared to the live database, measurements is done on both migrated data in the archive and the equivalent data in the live database.               

\subsection{Memory utilization}
The used metric originates from the internal metric defined as 'Maximum memory utilization' in ISO/IEC standard TR 9126-2 \cite{isoExternalMetric}. For the specific case of database applications, this metric can be interpreted as the size of the working set, meaning the amount of data that a database must keep in RAM in order to effectively perform a specific task. The working set is dependant on the kinds of queries that are typically performed as well as the overall disc usage of the database. Comparison is made of the estimated working sets for each of the databases used for implementing the archive against the working set of the relational database used by CIMS.

\subsection{Insertion time}
For the archive to be viable to use in practice, insertion of new documents into the archive must be efficient. Even with large number of documents already in the archive, the insertions must be made in adequate time. Measurements is be done on insertions of data originated from the live database, the metric will be applied on artifacts containing data sets of various sizes.

\subsection{Horizontal scalability}
The ability of the archive to scale horizontally is evaluated by qualitatively investigate each data stores' capability of scalability. In this case, capability is defined as the data store's characteristics in the following areas: easiness in adding new servers to the cluster, the amount of configuration needed for starting a cluster, and the flexibility in scaling meaning how much information of the data growth that is needed on beforehand.   

\section{Integration with CIMS}
In order to evaluate the suitability of the polyglot persistence approach, some new functionality that leverage the archive database will be built into CIMS. More specifically, we have chosen to implement the viewing of builds. This new functionality should work as follows; When a user tries requests the web page for a specific build, a first check is made to see if the build and its related data exists in the live relational database. If a build is found at this stage then its information is presented to the user as it normally would. If it is not however, a second check is made to see if the build exists in the archive database. If it does exist in the archive then it will be presented using the archive APIs.

\section{Organizational suitability}
The final point of evaluation is to assess how suitable a solution is from an organizational perspective. There are multiple factors to consider when evaluating the archives' suitability in the organization. Which roles at Ericsson are going to use the archive and how will they use it? What business value can be mined from the data in the archive? What obstacles exists when having both relational and non-relational databases in the same system and how expensive are these obstacles to overcome? Is the CIMS development team willing to learn a new technology to turn an archive prototype into a production ready system? Is the people who perform analytics willing to learn new query languages associated with NoSQL technology? How much overhead is required for maintaining two different data stores in CIMS? How suitable is the different prototypes in the current IT environment? Can the organization make use of all this data and be able to get something of value out from it? 
This point will be based on purely qualitative factors like the opinions of CIMS developers. We will discuss and with them and find out what the opinions are.

