\chapter{Evaluation}
\label{chap:eval}

In this chapter the different evaluation criteria of the artifact are described and motivated. The artifact is evaluated by metrics that capture the solution objectives and provide relevant data used to address research questions. The results of the evaluation are presented in the corresponding sections within chapter \ref{chap:results}.

\section{Data separation}
How well the separation of data works is evaluated qualitatively by reasoning about the general process of migrating data from the live database in CIMS to the database of the archive. This process is dependent on the effectiveness of the software components that are constructed to support the database of the archive.
A crucial part of this process is the transformation and denormalization of relational data. The process of separation is also dependent on how the archive database is configured and how suitable it is for the task. 

\section{Query support}
The usefulness of the archive depends on what queries it supports and how these perform at scale. As a basis for evaluation, the set of important queries defined previously in section~\ref{sec:usecases} is used. Evaluation is done by comparing this set to the set of queries implemented in the archive prototypes which are previously described in section~\ref{sec:archiveapi}.

%Comparison is made on the difference in the queries complexity. 

%This transformation is the foundation for how queries can be written against the archive. Depending on how the data is modeled in the archive, operations on that data will have different complexites.%\section{Schema evolution}
%The ability of the archive to handle schema evolution is evaluated by introducing realistic schema variation on incoming data to the archive. These changes are defined in collaboration with the CIMS development team and should capture the planned evolution of the database schema in CIMS.
%
%This evaluation is qualitative and is based on the "Adaptability of data structures" metric defined in \cite{isoInternalMetric}.

\section{Scalability}
The following points in this section focus on the ability of an archive prototype to scale well with current growth trends. This includes disk usage, memory utilization, insertion time and horizontal scalability.
%This evaluation will be done both quantitatively and qualitatively. The quantitative evaluation is done by measuring the size of the data in the archive after a migration. This value will then be compared to how much space this data took in the live database. Qualitatively, each data store's underlying ability to scale will be evaluated in means of which hardware and configuration is needed with a given total data size. Specifically, how much RAM is needed for specific data set sizes.

\subsection{Disk usage}
For the archive to scale, disk usage must be manageable, even with billions of documents in the archive. The metric used is derived from 'Memory utilization' defined in \cite{isoInternalMetric}, memory is in this case the disk space used by the database of the archive. In order to assess the archive's capacity for long term scalability compared to the live database, measurements are done on both migrated data in the archive and the equivalent data in a CIMS instance.

\subsection{Memory utilization}
The metric used originates from the internal metric defined as 'Maximum memory utilization' in ISO/IEC standard TR 9126-2 \cite{isoExternalMetric}. Measurement is made by using the archive to perform tasks like insertion and querying and estimating the amount of memory that is needed to perform the task adequately. 

\subsection{Insertion time}
For the archive to be viable in practice, insertion of new documents into the archive must be efficient. Even with a large number of documents already in the archive, the insertions must be made in adequate time, which in this case means faster insertions than the data generation rate for a CIMS instance. Currently, the CIMS instance for the largest product being developed produces about 2 million job events per day, and as already stated the trend is increasing (see figure~\ref{fig:events}).
Measurements are done on insertions of data originated from the live database and generated dummy data similar in size and structure to real data.

\subsection{Query response time}
To assess how performance of a prototype scales with the size of the data set, query response times will be measured on some important queries that an archive should support.

\subsection{Schema changes}
Since one of the archive prototypes (TokuDB) uses a relational database, its database schema must match the schema of the CIMS instance that feeds the archive with data. Therefore whenever the schema is changed in the CIMS instance, the schema in the archive must be changed as well. To see how well this operation performs with a large data set, schema changes will be done on the archive prototype and the running time of the operation will be measured. This evaluation point does not apply to Elasticsearch, since it is essentially schema-less.

\subsection{Horizontal scalability}
The ability of the archive to scale horizontally is evaluated by qualitatively investigating each constructed prototype's capability to do so. In this case, capability is defined as the database's characteristics in the following areas: the amount of upfront configuration needed to start a cluster and the amount of configuration needed to add new nodes to a running cluster.

\section{Integration with CIMS}
In order to evaluate the suitability of the polyglot persistence approach, some new functionality that leverages the archive database will be built into CIMS. More specifically, we have chosen to implement the viewing of builds. This new functionality should work as follows:
\begin{itemize}
    \item User tries requests the web page for a specific build.
    \item System checks to see if the build and its related data exist in the live relational database.
    \item If a build is found in the live database then it is presented to the user as it normally would.
    \item If a build is not found then the system checks if the build exists in the archive database. 
    \item If a build is found in the archive database then it is presented using the archive API. 
\end{itemize}

\section{Organizational suitability}
The final point of evaluation is to assess how suitable a solution is from an organizational perspective. There are multiple factors to consider when evaluating the archives' suitability in the organization. For example, who the target users of the archive are, willingness to learn and adopt new technology, willingness to make use of archived data, increased maintenance effort and system complexity, and so on. These points would be discussed with relevant people in the organization and would also be extended by observations made throughout the course of the study.

%Which roles at Ericsson are going to use the archive and how will they use it? What business value can be mined from the data in the archive? What obstacles exists when having both relational and non-relational databases in the same system and how expensive are these obstacles to overcome? Is the CIMS development team willing to learn a new technology to turn an archive prototype into a production ready system? Is the people who perform analytics willing to learn new query languages associated with NoSQL technology? How much overhead is required for maintaining two different data stores in CIMS? How suitable is the different prototypes in the current IT environment? Can the organization make use of all this data and be able to get something of value out from it? 
%This point will be based on purely qualitative factors like the opinions of CIMS developers. We will discuss and with them and find out what the opinions are.

